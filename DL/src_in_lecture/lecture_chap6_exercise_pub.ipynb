{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. Denoising Autoencoderの実装. また, MNISTを用いて次のことを確認\n",
    "- reconstruction errorが小さくなっている（学習が進んでいる）\n",
    "- 重みの可視化（特徴の可視化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "theano_rng = RandomStreams(rng.randint(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MNISTデータセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data.astype('float32'), mnist.target.astype('int32'))\n",
    "\n",
    "mnist_X = mnist_X / 255\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = np.eye(10)[train_y].astype('int32')\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Autoencoderをクラスとして定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    #- Constructor\n",
    "    def __init__(self, visible_dim, hidden_dim, function):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.function = function\n",
    "        self.W = theano.shared(rng.uniform(low=-0.08, high=0.08, size=(visible_dim, hidden_dim)).astype('float32'), name='W')\n",
    "        self.a = theano.shared(np.zeros(visible_dim).astype('float32'), name='a')\n",
    "        self.b = theano.shared(np.zeros(hidden_dim).astype('float32'), name='b')\n",
    "        self.params = [self.W, self.a, self.b]\n",
    "    \n",
    "    #- Encoder\n",
    "    def encode(self, x):\n",
    "        u = T.dot(x, self.W) + self.b# WRITE ME (HINT: use self.W and self.b)\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #- Decoder (Tied Weight)\n",
    "    def decode(self, x):\n",
    "        u = T.dot(x, self.W.T) + self.a # WRITE ME (HINT: use self.W and self.a)\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "    \n",
    "    #- Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        y = self.encode(x)# WRITE ME\n",
    "        reconst_x = self.decode(y)# WRITE ME\n",
    "        return reconst_x\n",
    "    \n",
    "    #- Reconstruction Error\n",
    "    def reconst_error(self, x, noise):\n",
    "        tilde_x = x * noise# WRITE ME (HINT: masking noise)\n",
    "        reconst_x = self.f_prop(tilde_x)\n",
    "        error = T.mean(T.sum(T.nnet.binary_crossentropy(reconst_x, x), axis=1))\n",
    "        return error, reconst_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 確率的勾配法 (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def sgd(params, g_params, eps=np.float32(1.0)):\n",
    "    updates = OrderedDict()\n",
    "    for param, g_param in zip(params, g_params):\n",
    "        updates[param] = param - eps*g_param\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. モデルの構築および学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Corruption level=0の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(train_X.shape[1], 500, T.nnet.sigmoid)\n",
    "\n",
    "x = T.fmatrix('x')\n",
    "noise = T.fmatrix('noise')\n",
    "\n",
    "cost, reconst_x = model.reconst_error(x, noise)\n",
    "params = model.params\n",
    "g_params = T.grad(cost=cost, wrt=params)\n",
    "updates = sgd(params, g_params)\n",
    "\n",
    "train = theano.function(inputs=[x, noise], outputs=[cost, reconst_x], updates=updates, allow_input_downcast=True, name='train')\n",
    "\n",
    "corruption_level = np.float32(0.0)\n",
    "batch_size = 100\n",
    "n_batches = train_X.shape[0] // batch_size\n",
    "\n",
    "#- Epoch\n",
    "for epoch in xrange(100):\n",
    "    train_X = shuffle(train_X)\n",
    "    err_all = []\n",
    "    for i in xrange(0, n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        noise = rng.binomial(size=train_X[start:end].shape, n=1, p=1-corruption_level)\n",
    "        err, reconst_x = train(train_X[start:end], noise)\n",
    "        err_all.append(err)\n",
    "    if epoch % 10 == 0:\n",
    "        print 'Epoch:%d, Error:%lf' % (epoch, np.mean(err))\n",
    "\n",
    "weight_0 = model.W.get_value(borrow=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Corruption level=0.5の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Autoencoder(train_X.shape[1], 500, T.nnet.sigmoid)\n",
    "\n",
    "x = T.fmatrix('x')\n",
    "noise = T.fmatrix('noise')\n",
    "\n",
    "cost, reconst_x = model.reconst_error(x, noise)\n",
    "params = model.params\n",
    "g_params = T.grad(cost=cost, wrt=params)\n",
    "updates = sgd(params, g_params)\n",
    "\n",
    "train = theano.function(inputs=[x, noise], outputs=[cost, reconst_x], updates=updates, allow_input_downcast=True, name='train')\n",
    "\n",
    "corruption_level = np.float32(0.5)\n",
    "batch_size = 100\n",
    "n_batches = train_X.shape[0] // batch_size\n",
    "\n",
    "#- Epoch\n",
    "for epoch in xrange(10):\n",
    "    train_X = shuffle(train_X)\n",
    "    err_all = []\n",
    "    for i in xrange(0, n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        noise = rng.binomial(size=train_X[start:end].shape, n=1, p=1-corruption_level)\n",
    "        err, reconst_x = train(train_X[start:end], noise)\n",
    "        err_all.append(err)\n",
    "    print 'Epoch:%d, Error:%lf' % (epoch, np.mean(err))\n",
    "\n",
    "weight_1 = model.W.get_value(borrow=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 重みの可視化\n",
    "- corruption_levelの違いによる重みの違いを観測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Corruption level=0の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in xrange(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight_0[i].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Corruption level=0.5の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in xrange(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(weight_1[i].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2. Stacked Denoising Autoencoder (SdA) の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SdA用のAutoencoderクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "    #- Constructor\n",
    "    def __init__(self, visible_dim, hidden_dim, W, function):\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim  = hidden_dim\n",
    "        self.function    = function\n",
    "        self.W           = W\n",
    "        self.a = theano.shared(np.zeros(visible_dim).astype('float32'), name='a')\n",
    "        self.b = theano.shared(np.zeros(hidden_dim).astype('float32'), name='b')\n",
    "        self.params = [self.W, self.a, self.b]\n",
    "\n",
    "    #- Encoder\n",
    "    def encode(self, x):\n",
    "        u = T.dot(x, self.W) + self.b\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "\n",
    "    #- Decoder\n",
    "    def decode(self, x):\n",
    "        u = T.dot(x, self.W.T) + self.a\n",
    "        y = self.function(u)\n",
    "        return y\n",
    "\n",
    "    #- Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        y = self.encode(x)\n",
    "        reconst_x = self.decode(y)\n",
    "        return reconst_x\n",
    "\n",
    "    #- Reconstruction Error\n",
    "    def reconst_error(self, x, noise):\n",
    "        tilde_x = x * noise\n",
    "        reconst_x = self.f_prop(tilde_x)\n",
    "        error = T.mean(T.sum(T.nnet.binary_crossentropy(reconst_x, x), axis=1))\n",
    "        return error, reconst_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SdA用のLayerクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    #- Constructor\n",
    "    def __init__(self, in_dim, out_dim, function):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.function = function\n",
    "        self.W = theano.shared(rng.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype('float32'), name='W')\n",
    "        self.b = theano.shared(np.zeros(out_dim).astype('float32'), name='b')\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.set_pretraining()\n",
    "\n",
    "    #- Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.u = T.dot(x, self.W) + self.b\n",
    "        self.z = self.function(self.u)\n",
    "        return self.z\n",
    "\n",
    "    #- Set Pretraining\n",
    "    def set_pretraining(self):\n",
    "        ae = Autoencoder(self.in_dim, self.out_dim, self.W, self.function)\n",
    "\n",
    "        x = T.fmatrix(name='x')\n",
    "        noise = T.fmatrix(name='noise')\n",
    "\n",
    "        cost, reconst_x = ae.reconst_error(x, noise)\n",
    "        params = ae.params\n",
    "        g_params = T.grad(cost=cost, wrt=params)\n",
    "        updates = sgd(params, g_params)\n",
    "\n",
    "        self.pretraining = theano.function(inputs=[x, noise], outputs=[cost, reconst_x], updates=updates, allow_input_downcast=True, name='pretraining')\n",
    "        hidden = ae.encode(x)\n",
    "        self.encode_function = theano.function(inputs=[x], outputs=hidden, allow_input_downcast=True, name='encode_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ネットワークの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [Layer(train_X.shape[1], 500,T.nnet.sigmoid),\n",
    "          Layer(500, 400, T.nnet.sigmoid),\n",
    "          Layer(400,10,T.nnet.softmax)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 事前学習 (Pre-training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining:: layer:0, Epoch:0, Error:88.959541\n",
      "Pretraining:: layer:0, Epoch:20, Error:69.456940\n",
      "Pretraining:: layer:0, Epoch:40, Error:70.779068\n",
      "Pretraining:: layer:0, Epoch:60, Error:68.347313\n",
      "Pretraining:: layer:0, Epoch:80, Error:63.713348\n",
      "Pretraining:: layer:0, Epoch:100, Error:69.128242\n",
      "Pretraining:: layer:0, Epoch:120, Error:66.695427\n",
      "Pretraining:: layer:0, Epoch:140, Error:65.421661\n",
      "Pretraining:: layer:0, Epoch:160, Error:65.632858\n",
      "Pretraining:: layer:0, Epoch:180, Error:63.464287\n",
      "Pretraining:: layer:0, Epoch:200, Error:66.786118\n",
      "Pretraining:: layer:0, Epoch:220, Error:62.673603\n",
      "Pretraining:: layer:0, Epoch:240, Error:63.581261\n",
      "Pretraining:: layer:0, Epoch:260, Error:66.324409\n",
      "Pretraining:: layer:0, Epoch:280, Error:66.932785\n",
      "Pretraining:: layer:0, Epoch:300, Error:64.789726\n",
      "Pretraining:: layer:0, Epoch:320, Error:63.827599\n",
      "Pretraining:: layer:0, Epoch:340, Error:65.669022\n",
      "Pretraining:: layer:0, Epoch:360, Error:63.904781\n",
      "Pretraining:: layer:0, Epoch:380, Error:65.705002\n",
      "Pretraining:: layer:0, Epoch:400, Error:65.819748\n",
      "Pretraining:: layer:0, Epoch:420, Error:64.930099\n",
      "Pretraining:: layer:0, Epoch:440, Error:63.947559\n",
      "Pretraining:: layer:0, Epoch:460, Error:64.918221\n",
      "Pretraining:: layer:0, Epoch:480, Error:65.673004\n",
      "Pretraining:: layer:0, Epoch:500, Error:64.214996\n",
      "Pretraining:: layer:1, Epoch:0, Error:105.391174\n",
      "Pretraining:: layer:1, Epoch:20, Error:82.284988\n",
      "Pretraining:: layer:1, Epoch:40, Error:80.893387\n",
      "Pretraining:: layer:1, Epoch:60, Error:76.635788\n",
      "Pretraining:: layer:1, Epoch:80, Error:76.946075\n",
      "Pretraining:: layer:1, Epoch:100, Error:77.365273\n",
      "Pretraining:: layer:1, Epoch:120, Error:77.032150\n",
      "Pretraining:: layer:1, Epoch:140, Error:76.804565\n",
      "Pretraining:: layer:1, Epoch:160, Error:76.411530\n",
      "Pretraining:: layer:1, Epoch:180, Error:77.661995\n",
      "Pretraining:: layer:1, Epoch:200, Error:77.145927\n",
      "Pretraining:: layer:1, Epoch:220, Error:79.116249\n",
      "Pretraining:: layer:1, Epoch:240, Error:77.662956\n",
      "Pretraining:: layer:1, Epoch:260, Error:77.696823\n",
      "Pretraining:: layer:1, Epoch:280, Error:77.179680\n",
      "Pretraining:: layer:1, Epoch:300, Error:75.450226\n",
      "Pretraining:: layer:1, Epoch:320, Error:76.610825\n",
      "Pretraining:: layer:1, Epoch:340, Error:76.729530\n",
      "Pretraining:: layer:1, Epoch:360, Error:75.016052\n",
      "Pretraining:: layer:1, Epoch:380, Error:76.132149\n",
      "Pretraining:: layer:1, Epoch:400, Error:78.037277\n",
      "Pretraining:: layer:1, Epoch:420, Error:77.307579\n",
      "Pretraining:: layer:1, Epoch:440, Error:77.450066\n",
      "Pretraining:: layer:1, Epoch:460, Error:78.780296\n",
      "Pretraining:: layer:1, Epoch:480, Error:75.847198\n",
      "Pretraining:: layer:1, Epoch:500, Error:77.411873\n"
     ]
    }
   ],
   "source": [
    "X = train_X\n",
    "for l, layer in enumerate(layers[:-1]):\n",
    "    corruption_level = np.float32(0.3)\n",
    "    batch_size = 100\n",
    "    n_batches = X.shape[0] // batch_size\n",
    "\n",
    "    for epoch in xrange(501):\n",
    "        X = shuffle(X)\n",
    "        err_all = []\n",
    "        for i in xrange(0, n_batches):\n",
    "            start = i*batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            noise = rng.binomial(size=X[start:end].shape, n=1, p=1-corruption_level)\n",
    "            err, reconst_x = layer.pretraining(X[start:end], noise)\n",
    "            err_all.append(err)\n",
    "        if epoch % 20 == 0:\n",
    "            print 'Pretraining:: layer:%d, Epoch:%d, Error:%lf' % (l, epoch, np.mean(err))\n",
    "    X = layer.encode_function(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. train関数, valid関数, test関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.fmatrix(name='x')\n",
    "t = T.imatrix(name='t')\n",
    "\n",
    "params = []\n",
    "for i, layer in enumerate(layers):\n",
    "    params += layer.params\n",
    "    if i == 0:\n",
    "        layer_out = layer.f_prop(x)\n",
    "    else:\n",
    "        layer_out = layer.f_prop(layer_out)\n",
    "\n",
    "y = layers[-1].z\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y, t))\n",
    "\n",
    "g_params = T.grad(cost=cost, wrt=params)\n",
    "updates = sgd(params, g_params)\n",
    "\n",
    "train = theano.function(inputs=[x, t], outputs=cost, updates=updates, allow_input_downcast=True, name='train')\n",
    "valid = theano.function(inputs=[x, t], outputs=[cost, T.argmax(y, axis=1)], allow_input_downcast=True, name='valid')\n",
    "test  = theano.function([x], T.argmax(y, axis=1), name='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 学習 (Fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:: 1, Validation cost: 0.262, Validation F1: 0.922\n",
      "EPOCH:: 21, Validation cost: 0.105, Validation F1: 0.972\n",
      "EPOCH:: 41, Validation cost: 0.106, Validation F1: 0.975\n",
      "EPOCH:: 61, Validation cost: 0.109, Validation F1: 0.976\n",
      "EPOCH:: 81, Validation cost: 0.111, Validation F1: 0.976\n",
      "EPOCH:: 101, Validation cost: 0.113, Validation F1: 0.976\n",
      "EPOCH:: 121, Validation cost: 0.115, Validation F1: 0.976\n",
      "EPOCH:: 141, Validation cost: 0.116, Validation F1: 0.976\n",
      "EPOCH:: 161, Validation cost: 0.118, Validation F1: 0.977\n",
      "EPOCH:: 181, Validation cost: 0.119, Validation F1: 0.977\n",
      "EPOCH:: 201, Validation cost: 0.120, Validation F1: 0.977\n",
      "EPOCH:: 221, Validation cost: 0.120, Validation F1: 0.977\n",
      "EPOCH:: 241, Validation cost: 0.121, Validation F1: 0.977\n",
      "EPOCH:: 261, Validation cost: 0.122, Validation F1: 0.977\n",
      "EPOCH:: 281, Validation cost: 0.123, Validation F1: 0.977\n",
      "EPOCH:: 301, Validation cost: 0.123, Validation F1: 0.977\n",
      "EPOCH:: 321, Validation cost: 0.124, Validation F1: 0.977\n",
      "EPOCH:: 341, Validation cost: 0.124, Validation F1: 0.977\n",
      "EPOCH:: 361, Validation cost: 0.125, Validation F1: 0.977\n",
      "EPOCH:: 381, Validation cost: 0.125, Validation F1: 0.977\n",
      "EPOCH:: 401, Validation cost: 0.126, Validation F1: 0.977\n",
      "EPOCH:: 421, Validation cost: 0.126, Validation F1: 0.977\n",
      "EPOCH:: 441, Validation cost: 0.126, Validation F1: 0.977\n",
      "EPOCH:: 461, Validation cost: 0.127, Validation F1: 0.978\n",
      "EPOCH:: 481, Validation cost: 0.127, Validation F1: 0.978\n",
      "EPOCH:: 501, Validation cost: 0.128, Validation F1: 0.977\n",
      "EPOCH:: 521, Validation cost: 0.128, Validation F1: 0.977\n",
      "EPOCH:: 541, Validation cost: 0.128, Validation F1: 0.977\n",
      "EPOCH:: 561, Validation cost: 0.129, Validation F1: 0.977\n",
      "EPOCH:: 581, Validation cost: 0.129, Validation F1: 0.977\n",
      "EPOCH:: 601, Validation cost: 0.129, Validation F1: 0.977\n",
      "EPOCH:: 621, Validation cost: 0.129, Validation F1: 0.977\n",
      "EPOCH:: 641, Validation cost: 0.130, Validation F1: 0.978\n",
      "EPOCH:: 661, Validation cost: 0.130, Validation F1: 0.978\n",
      "EPOCH:: 681, Validation cost: 0.130, Validation F1: 0.978\n",
      "EPOCH:: 701, Validation cost: 0.130, Validation F1: 0.978\n",
      "EPOCH:: 721, Validation cost: 0.131, Validation F1: 0.978\n",
      "EPOCH:: 741, Validation cost: 0.131, Validation F1: 0.978\n",
      "EPOCH:: 761, Validation cost: 0.131, Validation F1: 0.978\n",
      "EPOCH:: 781, Validation cost: 0.131, Validation F1: 0.978\n",
      "EPOCH:: 801, Validation cost: 0.131, Validation F1: 0.978\n",
      "EPOCH:: 821, Validation cost: 0.132, Validation F1: 0.978\n",
      "EPOCH:: 841, Validation cost: 0.132, Validation F1: 0.978\n",
      "EPOCH:: 861, Validation cost: 0.132, Validation F1: 0.978\n",
      "EPOCH:: 881, Validation cost: 0.132, Validation F1: 0.978\n",
      "EPOCH:: 901, Validation cost: 0.132, Validation F1: 0.978\n",
      "EPOCH:: 921, Validation cost: 0.133, Validation F1: 0.978\n",
      "EPOCH:: 941, Validation cost: 0.133, Validation F1: 0.978\n",
      "EPOCH:: 961, Validation cost: 0.133, Validation F1: 0.978\n",
      "EPOCH:: 981, Validation cost: 0.133, Validation F1: 0.978\n",
      "EPOCH:: 1001, Validation cost: 0.133, Validation F1: 0.978\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "\n",
    "for epoch in xrange(1001):\n",
    "    train_X, train_y = shuffle(train_X, train_y)\n",
    "    for i in xrange(n_batches):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        train(train_X[start:end], train_y[start:end])\n",
    "    if epoch % 20 == 0:\n",
    "        valid_cost, pred_y = valid(valid_X, valid_y)\n",
    "        print 'EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' % (epoch + 1, valid_cost, f1_score(np.argmax(valid_y, axis=1).astype('int32'), pred_y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
