{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. ロジスティック回帰の実装と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LogisticRegressionクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--- LogisticRegression\n",
    "class LogisticRegression:\n",
    "    #- Constructor\n",
    "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype(\"float32\")\n",
    "        self.b = np.zeros(out_dim).astype(\"float32\")\n",
    "        self.function = function\n",
    "        self.deriv_function = deriv_function\n",
    "        self.u = None\n",
    "        self.delta = None\n",
    "\n",
    "    #- Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.u = np.dot(x, self.W) + self.b\n",
    "        self.z = self.function(self.u)\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. シグモイド関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. データセットの設定とモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#- OR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [1]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "model = LogisticRegression(2, 1, sigmoid, deriv_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. train関数とtest関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピーともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, t, eps=1.0):\n",
    "    #- Forward Propagation\n",
    "    y = model.f_prop(X)\n",
    "    \n",
    "    #- Cost Function & Delta\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y)) # Negative Loglikelihood\n",
    "    delta = y - t\n",
    "    \n",
    "    #- Back Propagation\n",
    "    model.delta = delta\n",
    "    \n",
    "    #- Update Parameters\n",
    "    z = X\n",
    "    dW = np.dot(z.T, model.delta)\n",
    "    db = np.dot(np.ones(len(z)), model.delta)\n",
    "    model.W = model.W - eps*dW\n",
    "    model.b = model.b - eps*db\n",
    "    \n",
    "    #- Train Cost\n",
    "    y = model.f_prop(X)\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    return cost\n",
    "\n",
    "def test(X, t):\n",
    "    #- Test Cost\n",
    "    y = model.f_prop(X)\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99799688]\n",
      " [ 0.99798893]\n",
      " [ 0.00499169]\n",
      " [ 0.99999998]]\n"
     ]
    }
   ],
   "source": [
    "#- Epoch\n",
    "for epoch in xrange(1000):\n",
    "    #- Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "    \n",
    "print pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2. 活性化関数とその微分の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. シグモイド関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ソフトマックス関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis = 1, keepdims = True)\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tanh関数とその微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題3. 多層パーセプトロンの実装と学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Layerクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--- Layer\n",
    "class Layer:\n",
    "    #- Constructor\n",
    "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype(\"float32\")\n",
    "        self.b = np.zeros(out_dim).astype(\"float32\")\n",
    "        self.function = function\n",
    "        self.deriv_function = deriv_function\n",
    "        self.u = None\n",
    "        self.delta = None\n",
    "\n",
    "    #- Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.u = np.dot(x, self.W) + self.b\n",
    "        self.z = self.function(self.u)\n",
    "        return self.z\n",
    "    \n",
    "    #- Back Propagation\n",
    "    def b_prop(self, delta, W):\n",
    "        self.delta = np.dot(delta, W.T)*self.deriv_function(self.u)\n",
    "        return self.delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ネットワーク全体の順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_props(layers, x):\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.f_prop(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ネットワーク全体の誤差逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def b_props(layers, delta):\n",
    "    for i, layer in enumerate(layers[::-1]):\n",
    "        if i == 0:\n",
    "            layer.delta = delta\n",
    "        else:\n",
    "            delta = layer.b_prop(delta, _W)\n",
    "        _W = layer.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. データセットの設定とネットワークの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#- XOR\n",
    "train_X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "train_y = np.array([[1], [1], [0], [0]])\n",
    "test_X, test_y = train_X, train_y\n",
    "\n",
    "layers = [Layer(2, 3, sigmoid, deriv_sigmoid),\n",
    "          Layer(3, 1, sigmoid, deriv_sigmoid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. train関数とtest関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差関数\n",
    "* 負の対数尤度関数 (Negative Loglikelihood Function）\n",
    "* 交差エントロピーともいう\n",
    "\n",
    "$$ E ( {\\bf \\theta} ) =  -\\sum^N_{i=1} \\left[ t_i \\log y ({\\bf x}_i ; {\\bf \\theta}) + (1 - t_i) \\log \\{ 1 - y ({\\bf x}_i ; {\\bf \\theta}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, t, eps=1.0):\n",
    "    #- Forward Propagation\n",
    "    y = f_props(layers, X)\n",
    "    \n",
    "    #- Cost Function & Delta\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y)) # Negative Loglikelihood\n",
    "    delta = y - t\n",
    "    \n",
    "    #- Back Propagation\n",
    "    b_props(layers, delta)\n",
    "    \n",
    "    #- Update Parameters\n",
    "    z = X\n",
    "    for i, layer in enumerate(layers):\n",
    "        dW = np.dot(z.T, layer.delta)\n",
    "        db = np.dot(np.ones(len(z)), layer.delta)\n",
    "        layer.W = layer.W - eps*dW    \n",
    "        layer.b = layer.b - eps*db\n",
    "        z = layer.z\n",
    "        \n",
    "    #- Train Cost\n",
    "    y = f_props(layers, X)\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    return cost\n",
    "\n",
    "def test(X, t):\n",
    "    #- Test Cost\n",
    "    y = f_props(layers, X)\n",
    "    cost = np.sum(-t*np.log(y) - (1 - t)*np.log(1 - y))\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99788042]\n",
      " [ 0.99785468]\n",
      " [ 0.00182329]\n",
      " [ 0.0034414 ]]\n"
     ]
    }
   ],
   "source": [
    "#- Epoch\n",
    "for epoch in xrange(2000):\n",
    "    #- Online Learning\n",
    "    for x, y in zip(train_X, train_y):\n",
    "        cost = train(x[np.newaxis, :], y[np.newaxis, :])\n",
    "    cost, pred_y = test(test_X, test_y)\n",
    "    \n",
    "print pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
